param_dict:
  text_to_query:
    openai:
      model_params:
        engine: "mcd-insights-pro-4o" #"gpt_35_turbo" # gpt_35_turbo/gpt_test
        temperature: 0
        max_tokens: 1000
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 1000   
        top_p: 0.3  
        top_k : 10
        temperature: 0.5
        max_tries : 3
        stop: 
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:

  followup_question_tagging:
    openai:
      model_params:
        engine: "gpt_35_turbo" # gpt_35_turbo/gpt_test
        temperature: 0
        max_tokens: 1000
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 15
        max_tries: 3
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 1000   
        top_p: 0.3  
        top_k : 10
        temperature: 0.5
        max_tries : 3
        stop: 
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:

  query_to_chart_type:
    openai:
      model_params:
        engine: "gpt_35_turbo" # gpt_35_turbo / text-davinci-003 / gpt_test
        temperature: 0
        max_tokens:
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 1000   
        top_p: 0.3  
        top_k : 10
        temperature: 0.5
        max_tries : 3
        stop: 
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:

  query_to_chart_code:
    openai:
      model_params:
        engine: "gpt_35_turbo" # gpt_35_turbo / text-davinci-003 / gpt_test
        temperature: 0
        max_tokens:
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 1000   
        top_p: 0.3  
        top_k : 10
        temperature: 0.5
        max_tries : 3
        stop:
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:

  table_to_insight_questions:
    openai:
      model_params:
        engine: "gpt_test" # gpt_35_turbo / text-davinci-003
        temperature: 0
        max_tokens:
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3     
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 1000   
        top_p: 0.3  
        top_k : 10
        temperature: 0.5
        max_tries : 3
        stop:
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:


  questions_to_insights:
    openai:
      model_params:
        engine: "gpt_test" # gpt_35_turbo / text-davinci-003
        temperature: 0
        max_tokens:
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 1000   
        top_p: 0.3  
        top_k : 10
        temperature: 0.5
        max_tries : 3
        stop:
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:


  summarize_insights:
    openai:
      model_params:
        engine: "gpt_test" # gpt_35_turbo / text-davinci-003
        temperature: 0
        max_tokens:
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 1000   
        top_p: 0.3  
        top_k : 10
        temperature: 0.5
        max_tries : 3
        stop:
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:

  summarize_tables:
    openai:
      model_params:
        engine: "gpt_test" # gpt_35_turbo / text-davinci-003
        temperature: 0
        max_tokens:
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 1000   
        top_p: 0.3  
        top_k : 10
        temperature: 0.5
        max_tries : 3
        stop:
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:

  insight_questions_to_code:
    openai:
      model_params:
        engine: "gpt_test" # gpt_35_turbo / text-davinci-003
        temperature: 0
        max_tokens:
        n: 1
        # leave blank for None
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 519   
        top_p: 0.9      
        temperature: 0.5
        max_tries: 3
        stop:
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop:

  extract_formulas:
    openai:
      model_params:
        engine: "gpt_test" # gpt_35_turbo / text-davinci-003
        temperature: 0
        max_tokens:
        n: 1
        stop:
        function: "ChatCompletion"
        timeout: 60
        max_tries: 3
        history:
    aws_mistral:  #aws_mistral
      model_params:
        engine: "mistral.mistral-large-2402-v1:0" 
        temperature: 0
        max_tokens: 3000
        top_p : 0.1
        top_k : 10
        max_tries : 3
        stop:
    aws_claude_3:
      model_params:
        engine : "anthropic.claude-3-sonnet-20240229-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 1000
        top_k : 5
        max_tries: 3
        stop:
    aws_llama:
      model_params:
        engine : "meta.llama3-70b-instruct-v1:0"
        max_gen_len: 519   
        top_p: 0.9      
        temperature: 0.5
        max_tries: 3
        stop:
    aws_claude_2:
      model_params:
        engine: "anthropic.claude-v2:1"
        anthropic_version : "bedrock-2023-05-31"
        temperature: 0.5
        max_tokens_to_sample: 300
        top_p : 1
        top_k : 250
        max_tries : 3
        stop:
    aws_claude_3_5:
      model_params:
        engine : "anthropic.claude-3-5-sonnet-20240620-v1:0"
        anthropic_version : "bedrock-2023-05-31"
        max_tokens : 3000
        top_k : 10
        max_tries: 3
        stop: