{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import fsspec\n",
    "import re\n",
    "import posixpath as pp\n",
    "import pandas as pd \n",
    "import sqlite3\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from query_insights.utils import load_config, fs_connection, get_fs_and_abs_path\n",
    "from query_insights.utils import read_and_process_data, DotifyDict\n",
    "from query_insights.pre_processing import DBConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change accordingly \n",
    "file_names = [\"incidents.xlsx\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a config dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {'path': {\n",
    "                'data_dictionary_path': '../data/data_dictionary'},\n",
    "            'db_params': {'db_name': 'sqlite',\n",
    "                'sqlite_database_path': '../data/db/database.db',\n",
    "                'chunk_size': 500000},\n",
    "            'cloud_storage': {'platform': None,\n",
    "                'prefix_url': None,\n",
    "                'DefaultEndpointsProtocol': None,\n",
    "                'account_key_path': None,\n",
    "                'AccountName': None,\n",
    "                'EndpointSuffix': None}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DotifyDict(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_table_path = os.path.join(data_config.path.data_dictionary_path,\"raw\")\n",
    "output_json_file_path = data_config.path.data_dictionary_path\n",
    "cloud_storage = data_config.cloud_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### connection to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_url, storage_options = fs_connection(fs_connection_dict = cloud_storage, fs_key= None)\n",
    "\n",
    "_fs, _ = get_fs_and_abs_path(path=prefix_url, storage_options=storage_options)\n",
    "\n",
    "database_connection = DBConnection(\n",
    "            data_config = data_config,\n",
    "            fs =_fs,\n",
    "        )\n",
    "\n",
    "conn = database_connection.connection_db()\n",
    "\n",
    "for file_name in file_names:\n",
    "    table_name = file_name.split(\".\")[0]\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    text_columns = []\n",
    "    cursor = conn.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    for column_info in cursor:\n",
    "        if column_info[2] == 'TEXT':\n",
    "            text_columns.append(column_info[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to create and save a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_file_generator(\n",
    "    data_dict_table_path,\n",
    "    file_name,\n",
    "    output_json_file_path,\n",
    "    fs=None,\n",
    "    conn = conn,\n",
    "    text_columns = text_columns,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    generate a json file named `<file_name>.json` from a data dictionary csv/xlsx.\n",
    "    column_name, column_description are expected columns and if the id column is present,\n",
    "    the output json will have yes to the id key, and id key wont be there for non-id columns.\n",
    "    If the columns have less than 10 unique values they will be outputted in the json.\n",
    "\n",
    "    If there are any other columns in the csv/xlsx they will be outputted in the json as it is.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dict_table_path : str\n",
    "        input folder path containing the data dictionary (csv/xlsx) file\n",
    "    file_name : str\n",
    "        name of the data dictionary (csv/xlsx) file.\n",
    "    output_json_file_path : str\n",
    "        output folder path where the JSON files will be saved.\n",
    "    fs : fsspec.filesystem, optional\n",
    "        Filesystem of the url, by default ``None``\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # fs = fs or fsspec.filesystem(\"file\")\n",
    "    result_dict = {}\n",
    "    df = read_and_process_data(pp.join(data_dict_table_path, file_name))\n",
    "    \n",
    "    pattern = re.compile(r\"[^\\w]\")\n",
    "    \n",
    "    df['column_name'] = [col.lower().replace(\" \", \"_\") for col in df['column_name']]\n",
    "    df['column_name'] = [pattern.sub(\"_\", col) for col in df['column_name']]\n",
    "    \n",
    "    table_name = file_name.split(\".\")[0]\n",
    "\n",
    "    columns_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        column_dict = {}\n",
    "        for column, value in row.items():\n",
    "            if pd.notna(value):\n",
    "                if column == \"column_name\":\n",
    "                    column_dict[\"name\"] = value\n",
    "                elif column == \"column_description\":\n",
    "                    column_dict[\"description\"] = value\n",
    "                elif column == \"id\":\n",
    "                    column_dict[\"id\"] = \"Yes\"\n",
    "                else:\n",
    "                    \n",
    "                    column_dict[column] = value\n",
    "            # text_column and conn are passed here.\n",
    "            if column in text_columns:    \n",
    "                unique_values = pd.read_sql(f\"SELECT distinct {column} FROM {table_name}\", conn).values.ravel().tolist()\n",
    "                if len(unique_values) < 10:\n",
    "                    # unique_values = df[col].unique().tolist()\n",
    "                    column_dict['unique_values'] = unique_values\n",
    "                \n",
    "        columns_list.append(column_dict)\n",
    "        \n",
    "    result_dict = {\n",
    "        \"table_name\": table_name,\n",
    "        \"columns\": columns_list\n",
    "    }\n",
    "    \n",
    "    with fs.open(pp.join(output_json_file_path, f\"{result_dict['table_name']}.json\"), \"w\") as f:\n",
    "        json.dump(result_dict, f, indent=4, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    json_file_generator(data_dict_table_path, file_name, output_json_file_path,_fs, conn, text_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "query_insights",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
